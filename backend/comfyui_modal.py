"""
AI å•†å“è§†è§’è½¬æ¢ Web åº”ç”¨ - Modal åç«¯

åŸºäº Modal çš„æ— æœåŠ¡å™¨ GPU åç«¯ï¼Œè¿è¡Œ ComfyUI + Qwen-Image-Edit-2511 å·¥ä½œæµã€‚
æ ¸å¿ƒåŠŸèƒ½ï¼šç”¨æˆ·ä¸Šä¼ å•†å“/äººç‰©å›¾ç‰‡ï¼Œé€‰æ‹©å¤šä¸ªç›®æ ‡è§†è§’ï¼Œæ‰¹é‡ç”Ÿæˆå¤šå¼ ä¸åŒè§†è§’çš„å›¾ç‰‡ã€‚

Requirements:
- 5.1: Modal åº”ç”¨ä½¿ç”¨ L40S æˆ– A100 GPU
- 5.2: ä½¿ç”¨ @modal.cls è£…é¥°å™¨å’Œ scaledown_window ä¿æ´»
- 5.3: ä½¿ç”¨ @modal.enter è£…é¥°å™¨å¯åŠ¨ ComfyUI æœåŠ¡å™¨
- 5.4: ä½¿ç”¨ Modal volumes ç¼“å­˜æ¨¡å‹
- 5.5: ä» HuggingFace åŠ è½½æ¨¡å‹å¹¶ç¼“å­˜
- 5.6: æ”¯æŒ qwen_image_vae, qwen_2.5_vl_7b, Qwen-Image-Edit-2511
- 5.7: æ”¯æŒ Lightning LoRA æƒé‡
- 10.1: æ¨¡å‹åŠ è½½å¤±è´¥æ—¶è®°å½•é”™è¯¯å¹¶è¿”å›æœåŠ¡ä¸å¯ç”¨å“åº”
- 10.2: å›¾åƒç”Ÿæˆå¤±è´¥æ—¶è¿”å›æè¿°æ€§é”™è¯¯æ¶ˆæ¯
- 10.5: ç”Ÿæˆè¶…æ—¶åè¿”å›è¶…æ—¶é”™è¯¯
"""

import modal
import subprocess
import os
import shutil
import logging
from pathlib import Path
from typing import List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Modal åº”ç”¨é…ç½®
# ============================================================================

app = modal.App("qwen-image-edit")

# æ¨¡å‹ç¼“å­˜å· - é¿å…é‡å¤ä¸‹è½½æ¨¡å‹ (Requirement 5.4)
vol = modal.Volume.from_name("qwen-models", create_if_missing=True)

# ============================================================================
# Docker é•œåƒé…ç½®
# ============================================================================

# æ¨¡å‹ä¸‹è½½å‡½æ•° - åœ¨é•œåƒæ„å»ºæ—¶æ‰§è¡Œ
def download_models():
    """
    ä¸‹è½½æ‰€æœ‰æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•
    
    æ­¤å‡½æ•°åœ¨ Modal é•œåƒæ„å»ºæ—¶æ‰§è¡Œï¼Œå°†æ¨¡å‹ä¸‹è½½åˆ° /cache/models ç›®å½•ã€‚
    """
    from huggingface_hub import hf_hub_download
    import os
    from pathlib import Path
    
    cache_dir = Path("/cache/models")
    
    # æ¨¡å‹é…ç½®
    models = [
        # VAE
        {
            "name": "qwen_image_vae.safetensors",
            "repo_id": "Comfy-Org/Qwen-Image_ComfyUI",
            "filename": "split_files/vae/qwen_image_vae.safetensors",
            "local_dir": "vae",
        },
        # CLIP
        {
            "name": "qwen_2.5_vl_7b.safetensors",
            "repo_id": "Comfy-Org/HunyuanVideo_1.5_repackaged",
            "filename": "split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors",
            "local_dir": "clip",
        },
        # UNET
        {
            "name": "Qwen-Image-Edit-2511.safetensors",
            "repo_id": "Comfy-Org/Qwen-Image-Edit_ComfyUI",
            "filename": "split_files/diffusion_models/qwen_image_edit_2511_bf16.safetensors",
            "local_dir": "unet",
        },
        # LoRA - 4 steps
        {
            "name": "Qwen-Image-Lightning-4steps-V1.0.safetensors",
            "repo_id": "lightx2v/Qwen-Image-Lightning",
            "filename": "Qwen-Image-Lightning-4steps-V1.0.safetensors",
            "local_dir": "loras",
        },
        # LoRA - 8 steps
        {
            "name": "Qwen-Image-Lightning-8steps-V1.0.safetensors",
            "repo_id": "lightx2v/Qwen-Image-Lightning",
            "filename": "Qwen-Image-Lightning-8steps-V1.0.safetensors",
            "local_dir": "loras",
        },
    ]
    
    print("=" * 60)
    print("ğŸ“¦ ä¸‹è½½ Qwen-Image-Edit-2511 æ¨¡å‹...")
    print("=" * 60)
    
    for model in models:
        target_dir = cache_dir / model["local_dir"]
        target_dir.mkdir(parents=True, exist_ok=True)
        target_path = target_dir / model["name"]
        
        if target_path.exists():
            print(f"âœ… {model['name']} å·²å­˜åœ¨")
            continue
        
        print(f"ğŸ“¥ ä¸‹è½½ {model['name']}...")
        print(f"   ä»“åº“: {model['repo_id']}")
        print(f"   æ–‡ä»¶: {model['filename']}")
        
        try:
            downloaded_path = hf_hub_download(
                repo_id=model["repo_id"],
                filename=model["filename"],
                local_dir=str(target_dir),
                local_dir_use_symlinks=False,
            )
            
            # é‡å‘½åæ–‡ä»¶åˆ°ç›®æ ‡åç§°
            downloaded_path = Path(downloaded_path)
            if downloaded_path.name != model["name"]:
                import shutil
                shutil.move(str(downloaded_path), str(target_path))
                # æ¸…ç†ç©ºç›®å½•
                try:
                    for parent in downloaded_path.parents:
                        if parent == target_dir:
                            break
                        parent.rmdir()
                except OSError:
                    pass
            
            print(f"âœ… {model['name']} ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ {model['name']} ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    print("=" * 60)
    print("âœ… æ‰€æœ‰æ¨¡å‹ä¸‹è½½å®Œæˆ!")
    print("=" * 60)


image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install(
        "git",
        "wget",
        "aria2",
        "libgl1-mesa-glx",
        "libglib2.0-0",
    )
    .pip_install(
        "torch==2.1.0",
        "torchvision==0.16.0",
        "torchaudio==2.1.0",
        "comfy-cli",
        "fastapi",
        "uvicorn",
        "pydantic",
        "huggingface_hub",
        "Pillow",
    )
    .run_commands(
        # å®‰è£… ComfyUI
        "comfy --skip-prompt install --nvidia",
    )
    .run_commands(
        # å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹
        "cd /root/comfy/ComfyUI/custom_nodes && "
        "git clone https://github.com/lrzjason/Comfyui-QwenEditUtils && "
        "git clone https://github.com/city96/ComfyUI-GGUF && "
        "git clone https://github.com/ltdrdata/ComfyUI-Impact-Pack was-node-suite-comfyui && "
        "git clone https://github.com/yolain/ComfyUI-Easy-Use",
    )
)

# ============================================================================
# æ¨¡å‹è·¯å¾„é…ç½®
# ============================================================================

COMFYUI_ROOT = Path("/root/comfy/ComfyUI")
CACHE_ROOT = Path("/cache")

# æ¨¡å‹ç›®å½•æ˜ å°„ï¼šç¼“å­˜ç›®å½• -> ComfyUI ç›®å½•
MODEL_PATHS = {
    "vae": {
        "cache": CACHE_ROOT / "models" / "vae",
        "comfyui": COMFYUI_ROOT / "models" / "vae",
    },
    "clip": {
        "cache": CACHE_ROOT / "models" / "clip",
        "comfyui": COMFYUI_ROOT / "models" / "clip",
    },
    "unet": {
        "cache": CACHE_ROOT / "models" / "unet",
        "comfyui": COMFYUI_ROOT / "models" / "diffusion_models",  # ComfyUI ä½¿ç”¨ diffusion_models ç›®å½•
    },
    "loras": {
        "cache": CACHE_ROOT / "models" / "loras",
        "comfyui": COMFYUI_ROOT / "models" / "loras",
    },
}

# æ¨¡å‹æ–‡ä»¶åˆ—è¡¨ (Requirements 5.6, 5.7)
MODELS = {
    "vae": ["qwen_image_vae.safetensors"],
    "clip": ["qwen_2.5_vl_7b.safetensors"],
    "unet": ["Qwen-Image-Edit-2511.safetensors"],
    "loras": [
        "Qwen-Image-Lightning-4steps-V1.0.safetensors",
        "Qwen-Image-Lightning-8steps-V1.0.safetensors",
    ],
}

# ============================================================================
# é¢„è®¾è§†è§’æç¤ºè¯
# ============================================================================

PERSPECTIVE_PROMPTS = {
    "front": "Next Sceneï¼šæ­£é¢è§†è§’",
    "left_45": "Next Sceneï¼šå°†é•œå¤´å‘å·¦æ—‹è½¬45åº¦",
    "right_45": "Next Sceneï¼šå°†é•œå¤´å‘å³æ—‹è½¬45åº¦",
    "top_down": "Next Sceneï¼šå°†é•œå¤´è½¬ä¸ºä¿¯è§†",
    "bottom_up": "Next Sceneï¼šå°†é•œå¤´è½¬ä¸ºå¾®å¾®ä»°è§†",
    "close_up": "Next Sceneï¼šå°†é•œå¤´è½¬ä¸ºç‰¹å†™é•œå¤´",
    "wide_angle": "Next Sceneï¼šå°†é•œå¤´è½¬ä¸ºå¹¿è§’é•œå¤´",
    "move_forward": "Next Sceneï¼šå°†é•œå¤´å‘å‰ç§»åŠ¨",
    "move_backward": "Next Sceneï¼šå°†é•œå¤´å‘åç§»åŠ¨",
    "move_left": "Next Sceneï¼šå°†é•œå¤´å‘å·¦ç§»åŠ¨",
    "move_right": "Next Sceneï¼šå°†é•œå¤´å‘å³ç§»åŠ¨",
}


# ============================================================================
# ComfyUI æœåŠ¡ç±»
# ============================================================================

@app.cls(
    image=image,
    gpu="L40S",  # Requirement 5.1: L40S æˆ– A100 GPU
    scaledown_window=300,  # Requirement 5.2: 5åˆ†é’Ÿä¿æ´»
    volumes={"/cache": vol},  # Requirement 5.4: æ¨¡å‹ç¼“å­˜å·
    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶ï¼ˆæ”¯æŒå¤šå›¾ç”Ÿæˆï¼‰
)
class ComfyUI:
    """
    ComfyUI æœåŠ¡ç±»
    
    ä½¿ç”¨ Modal çš„ @modal.cls è£…é¥°å™¨é…ç½® GPU å’Œå®¹å™¨è®¾ç½®ã€‚
    ä½¿ç”¨ @modal.enter è£…é¥°å™¨åœ¨å®¹å™¨å¯åŠ¨æ—¶å¯åŠ¨ ComfyUI æœåŠ¡å™¨ã€‚
    
    Requirements:
    - 5.3: ä½¿ç”¨ @modal.enter è£…é¥°å™¨å¯åŠ¨ ComfyUI æœåŠ¡å™¨
    - 5.8: æš´éœ² FastAPI ç«¯ç‚¹
    """
    
    port: int = 8188
    comfyui_process: subprocess.Popen = None
    
    @modal.enter()
    def launch_comfy_background(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶å¯åŠ¨ ComfyUI æœåŠ¡å™¨ (Requirement 5.3)
        
        1. è®¾ç½®æ¨¡å‹ç¬¦å·é“¾æ¥
        2. å¯åŠ¨ ComfyUI åå°æœåŠ¡
        3. ç­‰å¾…æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡
        
        Requirements:
        - 10.1: æ¨¡å‹åŠ è½½å¤±è´¥æ—¶è®°å½•é”™è¯¯
        """
        import time
        
        print("=" * 60)
        print("ğŸš€ å¯åŠ¨ ComfyUI æœåŠ¡å™¨...")
        print("=" * 60)
        
        # 1. è®¾ç½®æ¨¡å‹ç¬¦å·é“¾æ¥
        try:
            self._setup_model_symlinks()
        except Exception as e:
            # Requirement 10.1: è®°å½•æ¨¡å‹åŠ è½½é”™è¯¯
            logger.error(f"Failed to setup model symlinks: {e}")
            raise RuntimeError(f"Model setup failed: {e}")
        
        # 2. å¯åŠ¨ ComfyUI åå°æœåŠ¡
        print(f"\nğŸ“¡ å¯åŠ¨ ComfyUI æœåŠ¡å™¨ (ç«¯å£: {self.port})...")
        
        # ä½¿ç”¨ comfy launch --background å¯åŠ¨
        cmd = f"comfy launch --background -- --port {self.port} --listen 127.0.0.1"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode != 0:
            error_msg = f"ComfyUI å¯åŠ¨å‘½ä»¤å¤±è´¥: {result.stderr}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            raise RuntimeError(f"Failed to start ComfyUI: {result.stderr}")
        
        print(f"âœ… ComfyUI å¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
        
        # 3. ç­‰å¾…æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡
        print("\nâ³ ç­‰å¾… ComfyUI æœåŠ¡å™¨å°±ç»ª...")
        try:
            self._poll_server_health(max_retries=60, delay=2.0)
        except RuntimeError as e:
            # Requirement 10.1: è®°å½•æœåŠ¡å™¨å¯åŠ¨å¤±è´¥
            logger.error(f"ComfyUI server health check failed: {e}")
            raise
        
        print("\n" + "=" * 60)
        print("âœ… ComfyUI æœåŠ¡å™¨å·²å°±ç»ª!")
        print("=" * 60)
    
    def _setup_model_symlinks(self):
        """
        å°†ç¼“å­˜çš„æ¨¡å‹é“¾æ¥åˆ° ComfyUI ç›®å½• (Requirement 5.5)
        
        è¿™æ ·å¯ä»¥é¿å…æ¯æ¬¡å®¹å™¨å¯åŠ¨æ—¶é‡æ–°ä¸‹è½½æ¨¡å‹ã€‚
        æ¨¡å‹ä» /cache/models ç›®å½•é“¾æ¥åˆ° ComfyUI çš„ models ç›®å½•ã€‚
        
        Requirements:
        - 10.1: æ¨¡å‹åŠ è½½å¤±è´¥æ—¶è®°å½•é”™è¯¯
        
        Raises:
            RuntimeError: å½“å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶ç¼ºå¤±æ—¶
        """
        print("\nğŸ”— è®¾ç½®æ¨¡å‹ç¬¦å·é“¾æ¥...")
        
        missing_models = []
        
        for model_type, paths in MODEL_PATHS.items():
            cache_dir = paths["cache"]
            comfyui_dir = paths["comfyui"]
            
            # ç¡®ä¿ ComfyUI ç›®å½•å­˜åœ¨
            comfyui_dir.mkdir(parents=True, exist_ok=True)
            
            # å¦‚æœç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Š
            if not cache_dir.exists():
                warning_msg = f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}"
                logger.warning(warning_msg)
                print(f"âš ï¸ {warning_msg}")
                # è®°å½•æ‰€æœ‰ç¼ºå¤±çš„æ¨¡å‹
                for model_file in MODELS.get(model_type, []):
                    missing_models.append(f"{model_type}/{model_file}")
                continue
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹æ–‡ä»¶åˆ›å»ºç¬¦å·é“¾æ¥
            for model_file in MODELS.get(model_type, []):
                cache_file = cache_dir / model_file
                comfyui_file = comfyui_dir / model_file
                
                if not cache_file.exists():
                    warning_msg = f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}"
                    logger.warning(warning_msg)
                    print(f"âš ï¸ {warning_msg}")
                    missing_models.append(f"{model_type}/{model_file}")
                    continue
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if comfyui_file.exists() or comfyui_file.is_symlink():
                    comfyui_file.unlink()
                
                # åˆ›å»ºç¬¦å·é“¾æ¥
                try:
                    os.symlink(cache_file, comfyui_file)
                    print(f"   âœ… {model_file} -> {comfyui_dir.name}/")
                except OSError as e:
                    error_msg = f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ {model_file}: {e}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€çš„æ¨¡å‹ç¼ºå¤±
        if missing_models:
            error_msg = f"ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {', '.join(missing_models)}"
            logger.error(error_msg)
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®© ComfyUI å¯åŠ¨æ—¶æŠ¥å‘Šå…·ä½“é”™è¯¯
            print(f"âš ï¸ {error_msg}")
            print("   è¯·è¿è¡Œ download_models_to_volume ä¸‹è½½æ¨¡å‹")
        
        print("ğŸ”— æ¨¡å‹ç¬¦å·é“¾æ¥è®¾ç½®å®Œæˆ")
    
    def _poll_server_health(self, max_retries: int = 60, delay: float = 2.0) -> bool:
        """
        æ£€æŸ¥ ComfyUI æœåŠ¡å™¨å¥åº·çŠ¶æ€
        
        è½®è¯¢ ComfyUI çš„ /system_stats ç«¯ç‚¹ï¼Œç›´åˆ°æœåŠ¡å™¨å°±ç»ªæˆ–è¶…æ—¶ã€‚
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            bool: æœåŠ¡å™¨æ˜¯å¦å¥åº·
            
        Raises:
            RuntimeError: æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶
        """
        import time
        import urllib.request
        import urllib.error
        
        url = f"http://127.0.0.1:{self.port}/system_stats"
        
        for i in range(max_retries):
            try:
                with urllib.request.urlopen(url, timeout=5) as response:
                    if response.status == 200:
                        print(f"   âœ… ComfyUI æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡ (å°è¯• {i + 1}/{max_retries})")
                        return True
            except (urllib.error.URLError, urllib.error.HTTPError) as e:
                if i % 5 == 0:  # æ¯ 5 æ¬¡æ‰“å°ä¸€æ¬¡çŠ¶æ€
                    print(f"   â³ ç­‰å¾…ä¸­... (å°è¯• {i + 1}/{max_retries})")
            except Exception as e:
                print(f"   âš ï¸ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            
            time.sleep(delay)
        
        raise RuntimeError(f"ComfyUI server failed to start after {max_retries * delay} seconds")
    
    def _check_server_health(self) -> bool:
        """
        æ£€æŸ¥ ComfyUI æœåŠ¡å™¨å½“å‰æ˜¯å¦å¥åº·
        
        Returns:
            bool: æœåŠ¡å™¨æ˜¯å¦å¥åº·
        """
        import urllib.request
        import urllib.error
        
        url = f"http://127.0.0.1:{self.port}/system_stats"
        
        try:
            with urllib.request.urlopen(url, timeout=5) as response:
                return response.status == 200
        except (urllib.error.URLError, urllib.error.HTTPError, Exception):
            return False
    
    # ========================================================================
    # å•å›¾æ¨ç†æ–¹æ³• (Requirement 6.5, 6.6)
    # ========================================================================
    
    @modal.method()
    def infer_single(
        self,
        input_image_base64: str,
        prompt: str,
        steps: int = 8,
        cfg: float = 3.0,
        seed: Optional[str] = None,
        output_prefix: str = "qwen_output",
    ) -> bytes:
        """
        æ‰§è¡Œå•ä¸ªå·¥ä½œæµå¹¶è¿”å›ç”Ÿæˆçš„å›¾ç‰‡
        
        Args:
            input_image_base64: Base64 ç¼–ç çš„è¾“å…¥å›¾ç‰‡
            prompt: ç”¨æˆ·æç¤ºè¯ï¼ˆè§†è§’æè¿°ï¼‰
            steps: ç”Ÿæˆæ­¥æ•° (4-8)
            cfg: CFG å¼ºåº¦ (1.0-5.0)
            seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
            
        Returns:
            bytes: ç”Ÿæˆçš„å›¾ç‰‡å­—èŠ‚æ•°æ®
            
        Raises:
            RuntimeError: å½“ ComfyUI æœåŠ¡å™¨ä¸å¥åº·æ—¶
            TimeoutError: å½“å·¥ä½œæµæ‰§è¡Œè¶…æ—¶æ—¶
            
        Requirements:
            - 6.5: ç¼©æ”¾è¾“å…¥å›¾ç‰‡ä½¿ç”¨ ImageScaleToTotalPixels
            - 6.6: ä½¿ç”¨ VAEDecode è§£ç  latent è¾“å‡ºä¸ºå›¾ç‰‡
            - 10.2: å›¾åƒç”Ÿæˆå¤±è´¥æ—¶è¿”å›æè¿°æ€§é”™è¯¯æ¶ˆæ¯
        """
        import base64
        import json
        import time
        import uuid
        import urllib.request
        import urllib.error
        from PIL import Image
        import io
        
        # ç¡®ä¿æœåŠ¡å™¨å¥åº· - Requirement 10.1
        if not self._check_server_health():
            error_msg = "ComfyUI server is not healthy"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        # 1. ä¿å­˜è¾“å…¥å›¾ç‰‡åˆ° ComfyUI input ç›®å½•
        client_id = uuid.uuid4().hex[:8]
        input_filename = f"input_{client_id}.png"
        input_path = COMFYUI_ROOT / "input" / input_filename
        input_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è§£ç  base64 å›¾ç‰‡å¹¶ä¿å­˜
        try:
            image_data = base64.b64decode(input_image_base64)
            with open(input_path, "wb") as f:
                f.write(image_data)
        except Exception as e:
            error_msg = f"Failed to save input image: {e}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        print(f"ğŸ“· è¾“å…¥å›¾ç‰‡å·²ä¿å­˜: {input_filename}")
        
        # 2. åˆ›å»ºå·¥ä½œæµ
        try:
            from .workflow_executor import create_workflow, save_workflow_to_file
            
            workflow = create_workflow(
                input_image=input_filename,
                prompt=prompt,
                steps=steps,
                cfg=cfg,
                seed=seed,
                output_prefix=f"{output_prefix}_{client_id}",
            )
        except Exception as e:
            error_msg = f"Failed to create workflow: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        # 3. ä¿å­˜å·¥ä½œæµåˆ°ä¸´æ—¶æ–‡ä»¶
        workflow_path = COMFYUI_ROOT / "temp" / f"workflow_{client_id}.json"
        workflow_path.parent.mkdir(parents=True, exist_ok=True)
        save_workflow_to_file(workflow, workflow_path)
        
        print(f"ğŸ“ å·¥ä½œæµå·²ä¿å­˜: {workflow_path.name}")
        
        # 4. é€šè¿‡ ComfyUI API æ‰§è¡Œå·¥ä½œæµ - Requirement 10.2, 10.5
        try:
            output_image = self._execute_workflow_via_api(workflow, client_id, output_prefix)
        except TimeoutError as e:
            logger.error(f"Workflow execution timed out: {e}")
            raise
        except Exception as e:
            error_msg = f"Workflow execution failed: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            input_path.unlink()
            workflow_path.unlink()
        except Exception as e:
            logger.warning(f"Failed to cleanup temp files: {e}")
        
        return output_image
    
    def _execute_workflow_via_api(
        self,
        workflow: dict,
        client_id: str,
        output_prefix: str,
        timeout: int = 120,
    ) -> bytes:
        """
        é€šè¿‡ ComfyUI API æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            workflow: å·¥ä½œæµå­—å…¸
            client_id: å®¢æˆ·ç«¯ ID
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            bytes: ç”Ÿæˆçš„å›¾ç‰‡å­—èŠ‚æ•°æ®
            
        Raises:
            TimeoutError: å½“å·¥ä½œæµæ‰§è¡Œè¶…æ—¶æ—¶ (Requirement 10.5)
            RuntimeError: å½“å·¥ä½œæµæ‰§è¡Œå¤±è´¥æ—¶ (Requirement 10.2)
        """
        import json
        import time
        import urllib.request
        import urllib.error
        
        api_url = f"http://127.0.0.1:{self.port}"
        
        # 1. æäº¤å·¥ä½œæµåˆ°é˜Ÿåˆ—
        prompt_data = {
            "prompt": workflow,
            "client_id": client_id,
        }
        
        try:
            req = urllib.request.Request(
                f"{api_url}/prompt",
                data=json.dumps(prompt_data).encode("utf-8"),
                headers={"Content-Type": "application/json"},
                method="POST",
            )
            
            with urllib.request.urlopen(req, timeout=10) as response:
                result = json.loads(response.read().decode("utf-8"))
                prompt_id = result.get("prompt_id")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                if "error" in result:
                    error_msg = f"ComfyUI rejected workflow: {result.get('error')}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
                    
        except urllib.error.URLError as e:
            error_msg = f"Failed to submit workflow to ComfyUI: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        except urllib.error.HTTPError as e:
            error_msg = f"ComfyUI API error: {e.code} - {e.reason}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        print(f"ğŸ“¤ å·¥ä½œæµå·²æäº¤: prompt_id={prompt_id}")
        
        # 2. è½®è¯¢ç­‰å¾…æ‰§è¡Œå®Œæˆ - Requirement 10.5
        start_time = time.time()
        last_status_check = 0
        
        while time.time() - start_time < timeout:
            elapsed = time.time() - start_time
            
            # æ¯ 10 ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
            if elapsed - last_status_check >= 10:
                print(f"   â³ ç­‰å¾…ç”Ÿæˆå®Œæˆ... ({int(elapsed)}s / {timeout}s)")
                last_status_check = elapsed
            
            # æ£€æŸ¥å†å²è®°å½•
            try:
                with urllib.request.urlopen(f"{api_url}/history/{prompt_id}", timeout=5) as response:
                    history = json.loads(response.read().decode("utf-8"))
                    
                    if prompt_id in history:
                        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                        status = history[prompt_id].get("status", {})
                        if status.get("status_str") == "error":
                            error_messages = status.get("messages", [])
                            error_msg = f"ComfyUI workflow failed: {error_messages}"
                            logger.error(error_msg)
                            raise RuntimeError(error_msg)
                        
                        outputs = history[prompt_id].get("outputs", {})
                        # æŸ¥æ‰¾ SaveImage èŠ‚ç‚¹çš„è¾“å‡º (node 80)
                        if "80" in outputs:
                            images = outputs["80"].get("images", [])
                            if images:
                                # è·å–ç¬¬ä¸€å¼ å›¾ç‰‡
                                image_info = images[0]
                                filename = image_info.get("filename")
                                subfolder = image_info.get("subfolder", "")
                                
                                print(f"   âœ… ç”Ÿæˆå®Œæˆ: {filename}")
                                
                                # ä» ComfyUI è·å–å›¾ç‰‡
                                return self._get_image_from_comfyui(filename, subfolder)
            except urllib.error.HTTPError as e:
                if e.code != 404:
                    logger.warning(f"Error checking history: {e}")
            except Exception as e:
                logger.warning(f"Error checking history: {e}")
            
            time.sleep(1)
        
        # Requirement 10.5: è¶…æ—¶é”™è¯¯
        error_msg = f"Workflow execution timed out after {timeout} seconds"
        logger.error(error_msg)
        raise TimeoutError(error_msg)
    
    def _get_image_from_comfyui(self, filename: str, subfolder: str = "") -> bytes:
        """
        ä» ComfyUI è·å–ç”Ÿæˆçš„å›¾ç‰‡
        
        Args:
            filename: å›¾ç‰‡æ–‡ä»¶å
            subfolder: å­æ–‡ä»¶å¤¹
            
        Returns:
            bytes: å›¾ç‰‡å­—èŠ‚æ•°æ®
        """
        import urllib.request
        import urllib.parse
        
        params = urllib.parse.urlencode({
            "filename": filename,
            "subfolder": subfolder,
            "type": "output",
        })
        
        url = f"http://127.0.0.1:{self.port}/view?{params}"
        
        with urllib.request.urlopen(url, timeout=30) as response:
            return response.read()
    
    # ========================================================================
    # æ‰¹é‡ç”Ÿæˆ API ç«¯ç‚¹ (Requirements 4.6, 4.7, 4.8, 4.9, 9.1, 9.2)
    # ========================================================================
    
    @modal.fastapi_endpoint(method="POST")
    def generate(self, request: dict):
        """
        API ç«¯ç‚¹ï¼šæ¥æ”¶æ‰¹é‡ç”Ÿæˆè¯·æ±‚
        
        POST /api/generate
        
        Request Body:
            {
                "image": "base64 encoded image",
                "perspectives": [
                    {"id": "left_45", "name": "å·¦ä¾§45Â°", "prompt": "Next Sceneï¼šå°†é•œå¤´å‘å·¦æ—‹è½¬45åº¦"},
                    ...
                ],
                "steps": 8,
                "cfg_scale": 3.0,
                "seed": "12345"  // optional
            }
        
        Response:
            {
                "images": [
                    {
                        "perspective_id": "left_45",
                        "perspective_name": "å·¦ä¾§45Â°",
                        "image": "base64 encoded result",
                        "seed_used": "12345"
                    },
                    ...
                ],
                "total_time": 12.5,
                "original_image": "base64 encoded original"
            }
        
        Requirements:
            - 4.6: éªŒè¯è¯·æ±‚å‚æ•°
            - 4.7: ä¸ºæ¯ä¸ªé€‰ä¸­çš„è§†è§’æ‰§è¡Œå·¥ä½œæµ
            - 4.8: æ”¯æŒæ‰¹é‡ç”Ÿæˆå¤šä¸ªè§†è§’
            - 4.9: è¿”å›æ‰€æœ‰ç”Ÿæˆçš„å›¾ç‰‡
            - 9.1: POST /api/generate ç«¯ç‚¹
            - 9.2: æ¥å— image, perspectives, steps, cfg_scale, seed å‚æ•°
            - 10.1: æ¨¡å‹åŠ è½½å¤±è´¥æ—¶è¿”å›æœåŠ¡ä¸å¯ç”¨å“åº”
            - 10.2: å›¾åƒç”Ÿæˆå¤±è´¥æ—¶è¿”å›æè¿°æ€§é”™è¯¯æ¶ˆæ¯
            - 10.5: ç”Ÿæˆè¶…æ—¶åè¿”å›è¶…æ—¶é”™è¯¯
        """
        import base64
        import time
        from fastapi import Response
        from fastapi.responses import JSONResponse
        
        start_time = time.time()
        
        print("\n" + "=" * 60)
        print("ğŸ“¥ æ”¶åˆ°ç”Ÿæˆè¯·æ±‚")
        print("=" * 60)
        
        # ====================================================================
        # 0. æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€ (Requirement 10.1)
        # ====================================================================
        if not self._check_server_health():
            logger.error("ComfyUI server is not healthy")
            return JSONResponse(
                status_code=503,
                content={
                    "error": "service_unavailable",
                    "message": "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
                }
            )
        
        # ====================================================================
        # 1. éªŒè¯è¯·æ±‚å‚æ•° (Requirement 4.6)
        # ====================================================================
        
        # éªŒè¯ image
        image_base64 = request.get("image")
        if not image_base64:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "validation_error",
                    "message": "image is required"
                }
            )
        
        # éªŒè¯ base64 å›¾ç‰‡æ ¼å¼
        try:
            # å°è¯•è§£ç  base64
            image_data = base64.b64decode(image_base64)
            if len(image_data) < 100:  # å¤ªå°ä¸å¯èƒ½æ˜¯æœ‰æ•ˆå›¾ç‰‡
                raise ValueError("Image data too small")
        except Exception as e:
            logger.warning(f"Invalid image data: {e}")
            return JSONResponse(
                status_code=400,
                content={
                    "error": "invalid_image",
                    "message": f"æ— æ•ˆçš„ base64 å›¾ç‰‡æ•°æ®: {str(e)}"
                }
            )
        
        # éªŒè¯ perspectives
        perspectives = request.get("perspectives", [])
        if not perspectives:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "validation_error",
                    "message": "at least one perspective is required"
                }
            )
        
        # éªŒè¯æ¯ä¸ª perspective çš„æ ¼å¼
        for i, p in enumerate(perspectives):
            if not isinstance(p, dict):
                return JSONResponse(
                    status_code=400,
                    content={
                        "error": "validation_error",
                        "message": f"perspective[{i}] must be an object"
                    }
                )
            if not p.get("prompt"):
                return JSONResponse(
                    status_code=400,
                    content={
                        "error": "validation_error",
                        "message": f"perspective[{i}].prompt is required"
                    }
                )
        
        # éªŒè¯ steps (4-8)
        steps = request.get("steps", 8)
        if not isinstance(steps, int) or steps < 4 or steps > 8:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "invalid_params",
                    "message": "steps must be an integer between 4 and 8"
                }
            )
        
        # éªŒè¯ cfg_scale (1.0-5.0)
        cfg_scale = request.get("cfg_scale", 3.0)
        if not isinstance(cfg_scale, (int, float)) or cfg_scale < 1.0 or cfg_scale > 5.0:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "invalid_params",
                    "message": "cfg_scale must be a number between 1.0 and 5.0"
                }
            )
        
        # è·å– seedï¼ˆå¯é€‰ï¼‰
        seed = request.get("seed")
        
        print(f"   ğŸ“Š å‚æ•°éªŒè¯é€šè¿‡:")
        print(f"      - è§†è§’æ•°é‡: {len(perspectives)}")
        print(f"      - Steps: {steps}")
        print(f"      - CFG Scale: {cfg_scale}")
        print(f"      - Seed: {seed or 'random'}")
        
        # ====================================================================
        # 2. æ‰¹é‡ç”Ÿæˆå›¾ç‰‡ (Requirements 4.7, 4.8)
        # ====================================================================
        
        generated_images = []
        
        for i, perspective in enumerate(perspectives):
            perspective_id = perspective.get("id", str(i))
            perspective_name = perspective.get("name", f"è§†è§’{i+1}")
            prompt = perspective.get("prompt", "")
            
            print(f"\n   ğŸ¨ ç”Ÿæˆè§†è§’ {i+1}/{len(perspectives)}: {perspective_name}")
            print(f"      Prompt: {prompt[:50]}...")
            
            try:
                # è°ƒç”¨å•å›¾æ¨ç†æ–¹æ³•
                img_bytes = self.infer_single.local(
                    input_image_base64=image_base64,
                    prompt=prompt,
                    steps=steps,
                    cfg=cfg_scale,
                    seed=seed,
                    output_prefix=f"qwen_{perspective_id}",
                )
                
                # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨
                generated_images.append({
                    "perspective_id": perspective_id,
                    "perspective_name": perspective_name,
                    "image": base64.b64encode(img_bytes).decode("utf-8"),
                    "seed_used": seed if seed else "random",
                })
                
                print(f"      âœ… ç”ŸæˆæˆåŠŸ ({len(img_bytes)} bytes)")
                
            except TimeoutError as e:
                # Requirement 10.5: è¶…æ—¶é”™è¯¯
                error_msg = f"è§†è§’ '{perspective_name}' çš„ç”Ÿæˆè¶…æ—¶"
                logger.error(f"Generation timeout for {perspective_name}: {e}")
                print(f"      âŒ {error_msg}")
                return JSONResponse(
                    status_code=504,
                    content={
                        "error": "timeout",
                        "message": error_msg
                    }
                )
            except RuntimeError as e:
                # Requirement 10.2: ç”Ÿæˆå¤±è´¥
                error_msg = f"è§†è§’ '{perspective_name}' çš„å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}"
                logger.error(f"Generation failed for {perspective_name}: {e}")
                print(f"      âŒ {error_msg}")
                return JSONResponse(
                    status_code=500,
                    content={
                        "error": "generation_error",
                        "message": error_msg
                    }
                )
            except Exception as e:
                # å…¶ä»–æœªçŸ¥é”™è¯¯
                error_msg = f"è§†è§’ '{perspective_name}' ç”Ÿæˆæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"
                logger.error(f"Unknown error for {perspective_name}: {e}")
                print(f"      âŒ {error_msg}")
                return JSONResponse(
                    status_code=500,
                    content={
                        "error": "generation_error",
                        "message": error_msg
                    }
                )
        
        # ====================================================================
        # 3. è¿”å›ç»“æœ (Requirement 4.9)
        # ====================================================================
        
        total_time = time.time() - start_time
        
        print(f"\n" + "=" * 60)
        print(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
        print(f"   - ç”Ÿæˆå›¾ç‰‡æ•°: {len(generated_images)}")
        print(f"   - æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print("=" * 60)
        
        return JSONResponse(
            content={
                "images": generated_images,
                "total_time": round(total_time, 2),
                "original_image": image_base64,
            },
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            }
        )


# ============================================================================
# æœ¬åœ°æµ‹è¯•å…¥å£
# ============================================================================

@app.local_entrypoint()
def main():
    """æœ¬åœ°æµ‹è¯•å…¥å£"""
    print("Modal app 'qwen-image-edit' is configured.")
    print(f"GPU: L40S")
    print(f"Scaledown window: 300 seconds")
    print(f"Volume: qwen-models mounted at /cache")


# ============================================================================
# æ¨¡å‹ä¸‹è½½å‡½æ•° (ç”¨äºé¢„çƒ­ç¼“å­˜å·)
# ============================================================================

@app.function(
    image=image,
    volumes={"/cache": vol},
    timeout=3600,  # 1å°æ—¶è¶…æ—¶ï¼Œæ¨¡å‹ä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
)
def download_models_to_volume():
    """
    ä¸‹è½½æ‰€æœ‰æ¨¡å‹åˆ° Modal Volume
    
    æ­¤å‡½æ•°ç”¨äºé¢„çƒ­ç¼“å­˜å·ï¼Œå°†æ¨¡å‹ä¸‹è½½åˆ° /cache/models ç›®å½•ã€‚
    è¿è¡Œæ–¹å¼: modal run backend/comfyui_modal.py::download_models_to_volume
    
    Requirements:
    - 5.5: ä» HuggingFace åŠ è½½æ¨¡å‹
    - 5.6: æ”¯æŒ qwen_image_vae, qwen_2.5_vl_7b, Qwen-Image-Edit-2511
    - 5.7: æ”¯æŒ Lightning LoRA æƒé‡
    """
    download_models()
    
    # æäº¤å·æ›´æ”¹
    vol.commit()
    print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° Modal Volume")


@app.function(
    image=image,
    volumes={"/cache": vol},
    timeout=60,
)
def verify_models_in_volume():
    """
    éªŒè¯ Modal Volume ä¸­çš„æ¨¡å‹æ–‡ä»¶
    
    è¿è¡Œæ–¹å¼: modal run backend/comfyui_modal.py::verify_models_in_volume
    """
    from pathlib import Path
    
    cache_dir = Path("/cache/models")
    
    print("ğŸ” éªŒè¯æ¨¡å‹æ–‡ä»¶...")
    print("=" * 60)
    
    all_exist = True
    for model_type, model_files in MODELS.items():
        print(f"\nğŸ“ {model_type}:")
        for model_file in model_files:
            file_path = cache_dir / model_type / model_file
            if file_path.exists():
                size_mb = file_path.stat().st_size / (1024 * 1024)
                print(f"   âœ… {model_file} ({size_mb:.1f} MB)")
            else:
                print(f"   âŒ {model_file} (ä¸å­˜åœ¨)")
                all_exist = False
    
    print("\n" + "=" * 60)
    if all_exist:
        print("âœ… æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²å°±ç»ª!")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·è¿è¡Œ download_models_to_volume ä¸‹è½½")
    print("=" * 60)
